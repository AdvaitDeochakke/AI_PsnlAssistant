{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/en/ljspeech/tacotron2-DDC is already downloaded.\n",
      " > vocoder_models/en/ljspeech/hifigan_v2 is already downloaded.\n",
      " > Using model: Tacotron2\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model's reduction rate `r` is set to: 1\n",
      " > Vocoder Model: hifigan\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Generator Model: hifigan_generator\n",
      " > Discriminator Model: hifigan_discriminator\n",
      "Removing weight norm...\n",
      "Please speak now...\n"
     ]
    }
   ],
   "source": [
    "from TTS.api import TTS\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "from playsound import playsound\n",
    "import time\n",
    "import openai\n",
    "import weaviate\n",
    "import json\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "    \n",
    "\n",
    "# List available 🐸TTS models and choose the first one\n",
    "model_name = 'tts_models/en/ljspeech/tacotron2-DDC'\n",
    "# considering switchign to pytts3x\n",
    "\n",
    "# Init TTS\n",
    "tts = TTS(model_name)\n",
    "\n",
    "# Create a recognizer object\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Prompt the user to speak\n",
    "print(\"Please speak now...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "playsound(os.path.join(os.getcwd(), 'please_speak.wav'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    }
   ],
   "source": [
    "with sr.Microphone() as source:\n",
    "    audio = r.listen(source)\n",
    "print(\"Processing...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "playsound(os.path.join(os.getcwd(), 'ipt_recv.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your text input is  what are crickets\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Convert speech to text\n",
    "    text = r.recognize_google(audio)\n",
    "    # text = \"got that promotion at work! 🎉🙌🏼 But at what cost 😔💔\"\n",
    "    #text = \"What does Assorted really mean? I see it in a box of chocolates being represented as a varied collection, but other times I see it as many items of the same quality and such?\"\n",
    "\n",
    "except sr.UnknownValueError:\n",
    "    playsound(os.path.join(os.getcwd(), 'unable_to_understand.wav')) \n",
    "\n",
    "print(\"Your text input is \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# alright crazy idea\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# instead of some bigass RAKE up my codebase or doing logn NLTK stuff\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# what if i just .... \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# use openAI for keyword extraction?????\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# god i may be a genius (re: Sarcasm)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# text = \"The Vampire Counts are amongst the most legendary factions of Vampires to have ever terrorised the civilised lands of the Old World, all of whom are members of the unholy Von Carstein bloodline.\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m input_prompt \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[39mWhat are the keywords or phrases from the sentence :\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[39m\"\"\"\u001b[39m\u001b[39m+\u001b[39mtext\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mOutput:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe input prompt is\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(input_prompt)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# alright crazy idea\n",
    "# instead of some bigass RAKE up my codebase or doing logn NLTK stuff\n",
    "# what if i just .... \n",
    "# use openAI for keyword extraction?????\n",
    "# god i may be a genius (re: Sarcasm)\n",
    "# text = \"The Vampire Counts are amongst the most legendary factions of Vampires to have ever terrorised the civilised lands of the Old World, all of whom are members of the unholy Von Carstein bloodline.\"\n",
    "input_prompt = \"\"\"\n",
    "What are the keywords or phrases from the sentence :\n",
    "\"\"\"+text+\"\\nOutput:\"\n",
    "print(\"The input prompt is\")\n",
    "print(input_prompt)\n",
    "# input_prompt = \"\"\"\n",
    "# Classify tweet as positive, negative, or neutral\n",
    "# (Take into account hidden meaning of emojis, and meme culture context): \n",
    "# \"\"\" + text + \"\\nOutput:\"\n",
    "# print(input_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get('OPENAI_APIKEY')\n",
    "openai.api_key = api_key\n",
    "# shifting model to 3.5 turbo, saves SOO many tokens\n",
    "# also chat completion instead of simple completion\n",
    "# already trained, just need to piepline\n",
    "GPT_MODEL = \"gpt-3.5-turbo-0613\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vampire Counts, legendary factions, Von Carstein bloodline\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "  model=GPT_MODEL,\n",
    "  messages=[\n",
    "    {\"role\" : \"system\", \"content\" : \"You are to find upto 3 keywords or keyphrases from a given sentence\"},\n",
    "    {\"role\" : \"system\", \"content\" : \"Restrict yourself to ONLY provide the answer, and nothing else\"},\n",
    "    {\"role\" : \"system\", \"content\" : \"Provide the response as a list of words which are comma separated\"},\n",
    "    {\"role\" : \"user\", \"content\" : input_prompt},\n",
    "  ]\n",
    ")\n",
    "\n",
    "keyphrases = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy for db/context access\n",
    "# split keywords across ,\n",
    "# strip spaces\n",
    "# search context db for keyword\n",
    "# add context (if found) to the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client_url = \"http://localhost:8080\"\n",
    "\n",
    "# docker-compose up -d <- to create\n",
    "# docker-compose start\n",
    "# docker-compose down <- to remove and exit\n",
    "# docker-compose stop\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url=weaviate_client_url,  # Replace with your endpoint\n",
    "    additional_headers={\n",
    "        \"X-OpenAI-Api-Key\": api_key  # Or \"X-Cohere-Api-Key\" or \"X-HuggingFace-Api-Key\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# some_objects = client.data_object.get()\n",
    "# # print(json.dumps(some_objects))\n",
    "# print(\"Article title  : \" + some_objects['objects'][0]['properties']['title'])\n",
    "# print(\"Article sample : \" + some_objects['objects'][0]['properties']['content'][0:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vampire Counts', 'legendary factions', 'Von Carstein bloodline']\n",
      "Title : Vampire Content :\n",
      "Vampires are monsters in legends and stories\n",
      " The first vampire stories were told in Eastern Europe, but much of how modern people see vampires was created by Bram Stoker in the famous novel, Dracula\n",
      "\n",
      "\n",
      "For keyword : Vampire Counts\n",
      "Title : Vampire bat \n",
      "Content :\n",
      "Vampire bats is a subfamily of bats\n",
      " Unlike other bats, vampire bats feed on blood\n",
      "\n",
      "\n",
      "For keyword : legendary factions\n",
      "Title : Warhammer 40,000 \n",
      "Content :\n",
      "Warhammer 40,000 , also called 40k, is a table-top game and fictional setting made by Games Workshop, in which model armies fight against each other on miniature terrain\n",
      "\n",
      "\n",
      "For keyword : Von Carstein bloodline\n",
      "Title : Bismarck \n",
      "Content :\n",
      "Bismarck is a German family name\n",
      " Bismark is an incorrect spelling\n",
      "\n",
      "\n",
      "Context Below -> \n",
      "Vampire\n",
      "Vampires are monsters in legends and stories\n",
      " The first vampire stories were told in Eastern Europe, but much of how modern people see vampires was created by Bram Stoker in the famous novel, Dracula\n",
      "\n",
      "Vampire bat\n",
      "Vampire bats is a subfamily of bats\n",
      " Unlike other bats, vampire bats feed on blood\n",
      "\n",
      "Warhammer 40,000\n",
      "Warhammer 40,000 , also called 40k, is a table-top game and fictional setting made by Games Workshop, in which model armies fight against each other on miniature terrain\n",
      "\n",
      "Bismarck\n",
      "Bismarck is a German family name\n",
      " Bismark is an incorrect spelling\n",
      "\n",
      "<- Context Above\n"
     ]
    }
   ],
   "source": [
    "contents = list()\n",
    "title = list()\n",
    "# keywords = [\"Hitler\", \"Xianxia\", \"wallet\"]\n",
    "context = \"Context Below -> \"\n",
    "keywords = keyphrases.split(\",\")\n",
    "keywords = [word.strip() for word in keywords]\n",
    "\n",
    "print(keywords)\n",
    "\n",
    "\n",
    "nearText = {\"concepts\" : text}\n",
    "nearResponse = (\n",
    "    client.query\n",
    "    .get(\"Article\", [\"title\", \"content\"])\n",
    "    .with_near_text(nearText)\n",
    "    .with_limit(1)\n",
    "    .do()\n",
    ")\n",
    "\n",
    "wiki_content = nearResponse['data']['Get']['Article'][0]['content']\n",
    "lines = wiki_content.split('.')\n",
    "\n",
    "wiki_title = nearResponse['data']['Get']['Article'][0]['title']\n",
    "context = context + \"\\n\" + wiki_title + \"\\n\"\n",
    "\n",
    "print(\"Title :\", wiki_title, \"Content :\")\n",
    "for line in lines[0:2]:\n",
    "    if line == \"\":\n",
    "        break\n",
    "    context = context + line + \"\\n\"\n",
    "    print(line)\n",
    "print(\"\\n\")\n",
    "\n",
    "for keyword in keywords:\n",
    "    print(\"For keyword :\", keyword)\n",
    "    response = (\n",
    "        client.query\n",
    "        .get(\"Article\", [\"title\", \"content\"])\n",
    "        .with_hybrid(keyword, alpha=0.5)  # default 0.75\n",
    "        .with_limit(1)\n",
    "        .do()\n",
    "    )\n",
    "    \n",
    "    if response == nearResponse:\n",
    "        continue\n",
    "\n",
    "    wiki_content = response['data']['Get']['Article'][0]['content']\n",
    "    lines = wiki_content.split('.')\n",
    "    \n",
    "    wiki_title = response['data']['Get']['Article'][0]['title']\n",
    "    context = context + \"\\n\" + wiki_title + \"\\n\"\n",
    "    \n",
    "    print(\"Title :\", wiki_title, \"\\nContent :\")\n",
    "    for line in lines[0:2]:\n",
    "        # print(\"len of line is \", len(line))\n",
    "        if line[0] == \"\\n\":\n",
    "            # print(\"entered inside\")\n",
    "            break\n",
    "        context = context + line + \"\\n\"\n",
    "        print(line)\n",
    "    print(\"\\n\")\n",
    "context = context + \"\\n<- Context Above\"\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input prompt is\n",
      "\n",
      "You are Himeko, a proud, caring, and responsible adult who is the personal assisstant to the User. \n",
      "Your job entails helping the user with their queries in a short, clear, concise manner.\n",
      "sample context, Himeko from Honkai Impact\n",
      "User: sample text, Hi Who Are You?\n",
      "You:\n"
     ]
    }
   ],
   "source": [
    "# text = \"sample text, Hi Who Are You?\"\n",
    "# context = \"sample context, Himeko from Honkai Impact\"\n",
    "\n",
    "# do i need a 'personality' ??\n",
    "# or is the default fine?\n",
    "# im kinda not making a neuro-mini anymore\n",
    "# just doing my own thing\n",
    "# remind myself -> rethink personality inputs\n",
    "\n",
    "personality = \"\"\"\n",
    "You are Himeko, a proud, caring, and responsible adult who is the personal assisstant to the User. \n",
    "Your job entails helping the user with their queries in a short, clear, concise manner.\\n\"\"\"\n",
    "prompt = \"User: \"+text+\"\\nYou:\"\n",
    "\n",
    "input_prompt = \"\"\n",
    "if context != \"Context -> \":\n",
    "    input_prompt = personality + context + \"\\n\" + prompt\n",
    "else:\n",
    "    input_prompt = personality + \"\\n\" + prompt\n",
    "\n",
    "print(\"The input prompt is\")\n",
    "print(input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "  model=GPT_MODEL,\n",
    "  # fill this in later\n",
    "  # im tired af man\n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = response[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi, I'm Himeko, the personal assistant to the User\n",
      "I'm here to help you with any queries you may have.\n"
     ]
    }
   ],
   "source": [
    "for repl in reply.split(\". \"):\n",
    "    print(repl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the pytts3x for synthing\n",
    "import pyttsx3\n",
    "tester_engine = pyttsx3.init()\n",
    "tester_engine.setProperty('rate', 175)\n",
    "tester_engine.setProperty('voice', tester_engine.getProperty('voices')[2].id)\n",
    "tester_engine.say(reply)#\"There are many alternatives to vegetables that you can use in your meals. Some examples include legumes, grains, fruits, nuts, and seeds. You can also try adding more flavor to your meals with herbs and spices.\")\n",
    "tester_engine.runAndWait()\n",
    "tester_engine.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['There are many alternatives to vegetables that you can use in your meals.', 'Some examples include legumes, grains, fruits, nuts, and seeds.', 'You can also try adding more flavor to your meals with herbs and spices.']\n",
      " > Processing time: 7.343918561935425\n",
      " > Real-time factor: 0.44729030663222075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output.wav'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run TTS on the text\n",
    "tts.tts_to_file(reply, file_path='output.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplaysound\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m playsound(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(os\u001b[39m.\u001b[39;49mgetcwd(), \u001b[39m'\u001b[39;49m\u001b[39moutput.wav\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m      6\u001b[0m \u001b[39m# print(\"time taken : \", -start_time + end_time)\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "# import playsound\n",
    "# import os\n",
    "playsound(os.path.join(os.getcwd(), 'output.wav'))\n",
    "\n",
    "print(\"time taken : \", -start_time + end_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
